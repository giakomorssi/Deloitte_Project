{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuiv3V2h7/2kmGipT2rz30",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giakomorssi/Deloitte_Project/blob/main/03_FraudDetection_noOS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the Data"
      ],
      "metadata": {
        "id": "t0315A0JjvY-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "ffbsL3U8jkV5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db9dd0f9-bc5e-4e0c-f61e-9ed688fc1427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Runtime switched to GPU\n",
            "GPU device not found\n",
            "Please install GPU version of TF\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Change Colab runtime to GPU\n",
        "import os\n",
        "os.environ['COLAB_TPU_ADDR'] = ''\n",
        "os.environ['COLAB_GPU_ALLOC'] = '1'\n",
        "os.environ['COLAB_GPU'] = '1'\n",
        "print(\"Runtime switched to GPU\")\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "if not tf.test.gpu_device_name():\n",
        "    print('GPU device not found')\n",
        "else:\n",
        "    print('GPU device found:', tf.test.gpu_device_name())\n",
        "\n",
        "# This code sets the runtime to use the GPU if available\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"Please install GPU version of TF\")\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/University/Deloitte/SupplyChainDataset.csv', encoding = 'latin-1')"
      ],
      "metadata": {
        "id": "aboYVnjYjxh5"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning"
      ],
      "metadata": {
        "id": "lYRBHmf-j3_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Na and Empty Columns\n",
        "\n",
        "df.drop(['Product Description', 'Order Zipcode', 'Order Profit Per Order', 'Customer Email', 'Customer Password'], axis = 1, inplace = True) \n",
        "df.dropna(inplace = True) #remove 1 missing value"
      ],
      "metadata": {
        "id": "rTjitksTj5vi"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Category Column"
      ],
      "metadata": {
        "id": "N1wlvoXlj8Mu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **COMPLETE:** The order or transaction has been successfully fulfilled and completed.\n",
        "2. **PENDING**: The order or transaction is still in progress and has not yet been completed.\n",
        "3. **CLOSED**: The order or transaction has been closed or terminated for some reason, such as a return or cancellation.\n",
        "4. **PENDING_PAYMENT**: The order or transaction is awaiting payment before it can be processed.\n",
        "5. **CANCELED**: The order or transaction has been canceled by the customer or the seller for some reason.\n",
        "6. **PROCESSING**: The order or transaction is being processed by the seller or merchant.\n",
        "7. **SUSPECTED_FRAUD**: The order or transaction is under review due to suspected fraudulent activity.\n",
        "8. **ON_HOLD**: The order or transaction has been placed on hold for some reason, such as a delay in shipping or a credit hold.\n",
        "9. **PAYMENT_REVIEW**: The payment for the order or transaction is under review by the payment processor or financial institution."
      ],
      "metadata": {
        "id": "1P3--qOaj_Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regular -> Complete, Pending, Pending_Payment, Processing\n",
        "# Suspected -> Closed, Canceled, On_Hold, Payment_Review\n",
        "# Fraud -> Suspected_Fraud\n",
        "\n",
        "# define dictionaries to map status values to categories\n",
        "regular_dict = {'COMPLETE': 'Regular', 'PENDING': 'Regular', 'PENDING_PAYMENT': 'Regular', 'PROCESSING': 'Regular'}\n",
        "suspected_dict = {'CLOSED': 'Suspected', 'CANCELED': 'Suspected', 'ON_HOLD': 'Suspected', 'PAYMENT_REVIEW': 'Suspected'}\n",
        "fraud_dict = {'SUSPECTED_FRAUD': 'Fraud'}\n",
        "\n",
        "# create a function to map status values to categories\n",
        "def map_category(status):\n",
        "    if status in regular_dict:\n",
        "        return regular_dict[status]\n",
        "    elif status in suspected_dict:\n",
        "        return suspected_dict[status]\n",
        "    elif status in fraud_dict:\n",
        "        return fraud_dict[status]\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "# apply the function to the 'status' column to create a new 'category' column\n",
        "df['Category'] = df['Order Status'].apply(map_category)\n",
        "\n",
        "print('Regular: ', len([x for x in df['Category'] if x == 'Regular']), '\\n')\n",
        "print('Suspected: ', len([x for x in df['Category'] if x == 'Suspected']), '\\n')\n",
        "print('Fraud: ', len([x for x in df['Category'] if x == 'Fraud']))"
      ],
      "metadata": {
        "id": "PErUmIXSj-dg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6711873-7500-4181-cef8-26e7a8abfb3c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regular:  141442 \n",
            "\n",
            "Suspected:  35004 \n",
            "\n",
            "Fraud:  4062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objs as go\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "\n",
        "temp = df[\"Category\"].value_counts()\n",
        "df1 = pd.DataFrame({'Category': temp.index,'values': temp.values})\n",
        "\n",
        "# Define a list of colors for the bars\n",
        "colors = ['red', 'blue', 'green']\n",
        "\n",
        "traces = []\n",
        "for i, category in enumerate(df1['Category']):\n",
        "    if category == 1:\n",
        "        name = \"Regular\"\n",
        "    elif category == 0:\n",
        "        name = \"Suspected\"\n",
        "    else:\n",
        "        name = \"Fraud\"\n",
        "    trace = go.Bar(\n",
        "        x=[name], y=[df1.loc[i, 'values']],\n",
        "        name=name,\n",
        "        marker=dict(color=colors[i]),\n",
        "        text=[df1.loc[i, 'values']],\n",
        "        legendgroup=\"group\"\n",
        "    )\n",
        "    traces.append(trace)\n",
        "\n",
        "layout = dict(title='Credit Card Fraud Class - data unbalance',\n",
        "              xaxis=dict(title='Class', showticklabels=True), \n",
        "              yaxis=dict(title='Number of transactions'),\n",
        "              hovermode='closest', width=600,\n",
        "              showlegend=True\n",
        "             )\n",
        "fig = go.Figure(data=traces, layout=layout)\n",
        "iplot(fig, filename='class')"
      ],
      "metadata": {
        "id": "2jPOt1hs2CKN",
        "outputId": "6c8f53cd-3ce8-41b5-dba8-c9a60727359c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"6783150f-12c9-4aec-af42-99585410182c\" class=\"plotly-graph-div\" style=\"height:525px; width:600px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6783150f-12c9-4aec-af42-99585410182c\")) {                    Plotly.newPlot(                        \"6783150f-12c9-4aec-af42-99585410182c\",                        [{\"legendgroup\":\"group\",\"marker\":{\"color\":\"red\"},\"name\":\"Fraud\",\"text\":[\"141442\"],\"x\":[\"Fraud\"],\"y\":[141442],\"type\":\"bar\"},{\"legendgroup\":\"group\",\"marker\":{\"color\":\"blue\"},\"name\":\"Fraud\",\"text\":[\"35004\"],\"x\":[\"Fraud\"],\"y\":[35004],\"type\":\"bar\"},{\"legendgroup\":\"group\",\"marker\":{\"color\":\"green\"},\"name\":\"Fraud\",\"text\":[\"4062\"],\"x\":[\"Fraud\"],\"y\":[4062],\"type\":\"bar\"}],                        {\"hovermode\":\"closest\",\"showlegend\":true,\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Credit Card Fraud Class - data unbalance\"},\"width\":600,\"xaxis\":{\"showticklabels\":true,\"title\":{\"text\":\"Class\"}},\"yaxis\":{\"title\":{\"text\":\"Number of transactions\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6783150f-12c9-4aec-af42-99585410182c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding"
      ],
      "metadata": {
        "id": "S_JKQJl9kC5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df.drop(['Category Name', 'Customer City',\n",
        "       'Customer Country', 'Customer Fname', 'Customer Id', 'Customer Lname',\n",
        "       'Customer State',\t'Customer Street', \n",
        "       'Order City', 'Order Country', 'Order Customer Id', 'Order Region',\t\n",
        "       'Order State', 'Product Image',\t'Product Name',\n",
        "       'shipping date (DateOrders)', 'order date (DateOrders)', 'Category Id', 'Customer Zipcode', \n",
        "       'Department Id', 'Latitude',\t'Longitude', 'Order Id',\t'Order Item Cardprod Id',\n",
        "       'Order Item Id', 'Product Card Id', 'Product Category Id'], axis = 1, inplace = True)\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/University/Deloitte/df_number.csv', index = False)"
      ],
      "metadata": {
        "id": "Zn7gWskckHxx"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split  \n",
        "\n",
        "df.drop(['Order Status'], axis=1, inplace=True)\n",
        "\n",
        "X = df.drop(['Category'], axis=1)\n",
        "y = df['Category']"
      ],
      "metadata": {
        "id": "eezGwCHSkNB9"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Type\n",
        "onehot = pd.get_dummies(X['Type'])\n",
        "X = pd.concat([X, onehot], axis=1)\n",
        "X.drop(['Type'], axis=1, inplace=True)\n",
        "\n",
        "# Department Name\n",
        "onehot = pd.get_dummies(X['Department Name'])\n",
        "X = pd.concat([X, onehot], axis=1)\n",
        "X.drop(['Department Name'], axis=1, inplace=True)\n",
        "\n",
        "# Market\n",
        "onehot = pd.get_dummies(X['Market'])\n",
        "X = pd.concat([X, onehot], axis=1)\n",
        "X.drop(['Market'], axis=1, inplace=True)\n",
        "\n",
        "# Customer Segment\n",
        "onehot = pd.get_dummies(X['Customer Segment'])\n",
        "X = pd.concat([X, onehot], axis=1)\n",
        "X.drop(['Customer Segment'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "pQrkuJkenwf0"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True)"
      ],
      "metadata": {
        "id": "4KK51NNrqhm8"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Shipping Mode\n",
        "custom_order = ['Same Day', 'First Class', 'Second Class', 'Standard Class']\n",
        "le.fit(custom_order)\n",
        "X_train['Shipping Mode'] = le.fit_transform(X_train['Shipping Mode'])\n",
        "X_test['Shipping Mode'] = le.transform(X_test['Shipping Mode'])\n",
        "\n",
        "# Delivery Status\n",
        "# Define the custom order\n",
        "custom_order = ['Shipping on time', 'Advance shipping', 'Late delivery', 'Shipping canceled']\n",
        "le.fit(custom_order)\n",
        "X_train['Delivery Status'] = le.fit_transform(X_train['Delivery Status'])\n",
        "X_test['Delivery Status'] = le.transform(X_test['Delivery Status'])\n",
        "\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)"
      ],
      "metadata": {
        "id": "qaz4tUWZkFKd"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define the column transformer to apply scaling only to non-one-hot-encoded columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), list(range(0, 16)))\n",
        "    ],\n",
        "    remainder=\"passthrough\"\n",
        ")\n",
        "\n",
        "column_names = list(X_train.columns)\n",
        "\n",
        "# Create a pipeline with the preprocessor and any other desired transformers\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    # add other transformers or classifiers as desired\n",
        "])\n",
        "\n",
        "# Fit and transform the input data using the pipeline\n",
        "X_train = pipeline.fit_transform(X_train)\n",
        "X_test = pipeline.transform(X_test)\n",
        "\n",
        "X_train = pd.DataFrame(X_train, columns=column_names)\n",
        "X_test = pd.DataFrame(X_test, columns=column_names)"
      ],
      "metadata": {
        "id": "o6j623udkPNc"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X_train = pd.DataFrame(X_train)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "y_train = np.ravel(y_train)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(type(X), type(y))"
      ],
      "metadata": {
        "id": "1ZKOgCbeELOn",
        "outputId": "3da67638-c973-4a91-c1e1-6111aafde0e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(144406, 39) (144406,)\n",
            "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.series.Series'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA"
      ],
      "metadata": {
        "id": "DrsgUBFskMG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize a PCA object\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA object to the data\n",
        "pca.fit(X_train)\n",
        "\n",
        "# Create a scree plot\n",
        "num_components = len(pca.explained_variance_ratio_)\n",
        "plt.plot(np.arange(1, num_components+1), pca.explained_variance_ratio_, 'ro-', linewidth=2)\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Proportion of Variance Explained')\n",
        "plt.show()\n",
        "\n",
        "# Determine the number of components to keep\n",
        "variance_threshold = 0.95\n",
        "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "num_components_to_keep = np.argmax(cumulative_variance_ratio >= variance_threshold) + 1\n",
        "\n",
        "print(f'\\n Number of components to keep: {num_components_to_keep}')\n",
        "\n",
        "# Transform the data using the chosen number of components\n",
        "pca = PCA(n_components=num_components_to_keep)\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)"
      ],
      "metadata": {
        "id": "QJk2p_EOkR7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the cumulative variance explained\n",
        "\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "cum = np.insert(cumulative_variance_ratio, 0, 0)\n",
        "ylab = np.insert(np.cumsum(pca.explained_variance_ratio_), 0, 0)\n",
        "\n",
        "plt.plot(cum, 'ro-', linewidth=2)\n",
        "plt.title('Cumulative Variance Explained')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Variance Explained')\n",
        "plt.yticks(ylab)\n",
        "plt.xticks(np.arange(0, 21))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kO_ObeGmkT1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Heatmap loadings\n",
        "fig = plt.figure(figsize=(20, 10))\n",
        "sns.heatmap(pca.components_, cmap='coolwarm', annot=True, cbar=False)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Principal Components')\n",
        "plt.title('PCA Loadings Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7fLYQwRlkViD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the principal components as vectors in the original feature space\n",
        "pc_vectors = pca.components_\n",
        "\n",
        "# Get the names of the original columns\n",
        "column_names = X.columns\n",
        "\n",
        "# Print the names of the columns chosen as principal components\n",
        "num_pcs = pc_vectors.shape[0]\n",
        "for i in range(num_pcs):\n",
        "    pc_name = f'PC{i+1}'\n",
        "    pc_loadings = pc_vectors[i]\n",
        "    relevant_columns = column_names[np.abs(pc_loadings) >= 0.40]\n",
        "    print(f'{pc_name}:\\n {relevant_columns.tolist()}, \\n {pc_loadings[np.abs(pc_loadings) >= 0.40]} \\n ')"
      ],
      "metadata": {
        "id": "3rjD9okVkXI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA MCA"
      ],
      "metadata": {
        "id": "tB47s_04FOjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize a PCA object\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA object to the data\n",
        "pca.fit(X_train.iloc[:, :16])\n",
        "\n",
        "# Determine the number of components to keep\n",
        "variance_threshold = 0.95\n",
        "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "num_components_to_keep = np.argmax(cumulative_variance_ratio >= variance_threshold) + 1\n",
        "\n",
        "print(f'\\n Number of components to keep: {num_components_to_keep}')\n",
        "\n",
        "# Transform the data using the chosen number of components\n",
        "pca = PCA(n_components=num_components_to_keep)\n",
        "pca_train = pca.fit_transform(X_train.iloc[:, :16])\n",
        "#pca_test = pca.transform(X_test[:, :16])"
      ],
      "metadata": {
        "id": "au3huJShFQyl",
        "outputId": "f34e89fb-0da5-4601-da06-d799b04afe5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Number of components to keep: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from prince import MCA\n",
        "\n",
        "# Separate the categorical variables\n",
        "categorical_data = X_train.iloc[:, 16:]\n",
        "\n",
        "# Initialize the MCA model with the desired number of components\n",
        "mca = MCA(n_components=2)\n",
        "\n",
        "# Fit the model to the categorical data\n",
        "mca.fit(categorical_data)\n",
        "\n",
        "# Transform the data into the new reduced dimensions\n",
        "mca_train = mca.transform(categorical_data)\n",
        "\n",
        "# The transformed data is now in a new 2D space, with the columns representing the new dimensions"
      ],
      "metadata": {
        "id": "0peNRqVtFlMO",
        "outputId": "242d2b9e-e9c6-4cc0-8319-383720c75633",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: prince in /usr/local/lib/python3.9/dist-packages (0.10.2)\n",
            "Requirement already satisfied: pandas<3.0.0,>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from prince) (1.5.3)\n",
            "Requirement already satisfied: altair<5.0.0,>=4.2.2 in /usr/local/lib/python3.9/dist-packages (from prince) (4.2.2)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from prince) (1.2.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair<5.0.0,>=4.2.2->prince) (0.12.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair<5.0.0,>=4.2.2->prince) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair<5.0.0,>=4.2.2->prince) (4.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from altair<5.0.0,>=4.2.2->prince) (3.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from altair<5.0.0,>=4.2.2->prince) (1.22.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas<3.0.0,>=1.4.1->prince) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas<3.0.0,>=1.4.1->prince) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn<2.0.0,>=1.0.2->prince) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn<2.0.0,>=1.0.2->prince) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn<2.0.0,>=1.0.2->prince) (1.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair<5.0.0,>=4.2.2->prince) (0.19.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair<5.0.0,>=4.2.2->prince) (22.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas<3.0.0,>=1.4.1->prince) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->altair<5.0.0,>=4.2.2->prince) (2.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "P-NJnauSkf84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = DecisionTreeClassifier(class_weight='balanced', max_depth = 4)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open('model.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "3YIFaGN7km9-"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score\n",
        "import pickle\n",
        "\n",
        "with open('model.pkl', 'rb') as file:\n",
        "    model = pickle.load(file)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print('Confusion Matrix\\n', confusion_matrix(y_test, model.predict(X_test)), '\\n')\n",
        "print('Recalls', recall_score(y_test, y_pred, average=None))\n",
        "print('Precisions', precision_score(y_test, y_pred, average=None))\n",
        "print('Accuracy', accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "a1uImn8w_aBW",
        "outputId": "f27835a6-71f5-4e84-bd87-f07612405c96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix\n",
            " [[  812     0     0]\n",
            " [    0 28289     0]\n",
            " [  736  2315  3950]] \n",
            "\n",
            "Recalls [1.     1.     0.5642]\n",
            "Precisions [0.5245 0.9244 1.    ]\n",
            "Accuracy 0.9154894465680572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "OCrPFvJokpeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix, recall_score\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Standardize the data and split it into training and test sets\n",
        "s = StandardScaler()\n",
        "le = LabelEncoder()\n",
        "\n",
        "recall_scores = []\n",
        "precision_scores = []\n",
        "fraud_recall = []\n",
        "suspected_recall = []\n",
        "regular_recall = [] \n",
        "low = []\n",
        "avg_conf_matrix = np.zeros((3, 3))\n",
        "\n",
        "for i in range(1, 51):\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True)\n",
        "\n",
        "  # Shipping Mode\n",
        "  custom_order = ['Same Day', 'First Class', 'Second Class', 'Standard Class']\n",
        "  le.fit(custom_order)\n",
        "  X_train['Shipping Mode'] = le.fit_transform(X_train['Shipping Mode'])\n",
        "  X_test['Shipping Mode'] = le.transform(X_test['Shipping Mode'])\n",
        "\n",
        "  # Delivery Status\n",
        "  # Define the custom order\n",
        "  custom_order = ['Shipping on time', 'Advance shipping', 'Late delivery', 'Shipping canceled']\n",
        "  le.fit(custom_order)\n",
        "  X_train['Delivery Status'] = le.fit_transform(X_train['Delivery Status'])\n",
        "  X_test['Delivery Status'] = le.transform(X_test['Delivery Status'])\n",
        "\n",
        "  # Category\n",
        "  y_train = le.fit_transform(y_train)\n",
        "  y_test = le.transform(y_test)\n",
        "\n",
        "  preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), list(range(0, 16)))\n",
        "    ],\n",
        "    remainder=\"passthrough\")\n",
        "\n",
        "  pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor)])\n",
        "\n",
        "  X_train = pipeline.fit_transform(X_train)\n",
        "  X_test = pipeline.transform(X_test)\n",
        "\n",
        "\n",
        "  # PCA\n",
        "\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  recall_scores.append(recall_score(y_test, y_pred, average=None))\n",
        "  fraud_recall.append(recall_score(y_test, y_pred, average=None)[0])\n",
        "  regular_recall.append(recall_score(y_test, y_pred, average=None)[1])\n",
        "  suspected_recall.append(recall_score(y_test, y_pred, average=None)[2])\n",
        "  precision_scores.append(precision_score(y_test, y_pred, average=None))\n",
        "\n",
        "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "  avg_conf_matrix += conf_matrix\n",
        "\n",
        "  if i % 10 == 0:\n",
        "      print(f'Iteration: {i}')\n",
        "      print(f'Fraud Recall {round(np.average(fraud_recall), 4)}, {round(np.std(fraud_recall), 4)}')\n",
        "      print(f'Suspected Recall {round(np.average(suspected_recall), 4)}, {round(np.std(suspected_recall), 4)}')\n",
        "      print(f'Regular Recall {round(np.average(regular_recall), 4)}, {round(np.std(regular_recall), 4)}')\n",
        "      print(f'Total Recall {round(np.average(recall_scores), 4)}, {round(np.std(recall_scores), 4)} \\n')\n",
        "\n",
        "  if recall_score(y_test, y_pred, average=None)[0] < 0.7:\n",
        "    low.append(round(recall_score(y_test, y_pred, average=None)[0], 4))\n",
        "  \n",
        "print(f'\\n Fraud Recall: {round(np.average(fraud_recall), 4)}, std: {round(np.std(fraud_recall), 4)}, Under 0.7: {len(low)}, {low}\\n Suspected Recall: {round(np.average(suspected_recall), 4)}, std: {round(np.std(suspected_recall), 4)}\\n Regular Recall: {round(np.average(regular_recall), 4)}, std: {round(np.std(regular_recall), 4)}\\n Total: {round(np.average(recall_scores), 4)}, std: {round(np.std(recall_scores), 4)}')\n",
        "\n",
        "np.set_printoptions(precision=4)\n",
        "avg_conf_matrix /= 20\n",
        "print(\"\\n Average Confusion Matrix:\")\n",
        "print(avg_conf_matrix)\n",
        "\n",
        "print('Precisions', precision_score(y_test, y_pred, average=None))\n",
        "\n",
        "print('\\n', model)"
      ],
      "metadata": {
        "id": "YWk3Dt0hkrJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synthetic Data"
      ],
      "metadata": {
        "id": "plqxS6GvUrRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.replace({0: 'Fraud', 1: 'Regular', 2: 'Suspected'}, inplace=True)"
      ],
      "metadata": {
        "id": "y6C_lswQdeI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X_test.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)\n",
        "test = pd.concat([X_test, y_test], axis=1)\n",
        "test"
      ],
      "metadata": {
        "id": "k0B2eUeocloa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/sdv-dev/CTGAN.git"
      ],
      "metadata": {
        "id": "cf58lz7WhBJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.select_dtypes(exclude=\"number\").columns.tolist()"
      ],
      "metadata": {
        "id": "iXOpyc4Kl4IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4ZaYzbIUl8T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ctgan import CTGAN\n",
        "import pandas as pd\n",
        "\n",
        "categoricals = test.select_dtypes(exclude=\"number\").columns.tolist()\n",
        "\n",
        "\n",
        "ctgan = CTGAN(epochs=10)\n",
        "ctgan.fit(test[:-1], categoricals)\n",
        "\n",
        "synthetic_data = ctgan.sample(len(test))"
      ],
      "metadata": {
        "id": "7ioy7LJygxFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "veIJs4BOluYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from ctgan import CTGAN\n",
        "\n",
        "categoricals = df.select_dtypes(exclude=\"number\").columns.tolist()\n",
        "\n",
        "# Fit CTGAN\n",
        "ctgan = CTGAN(epochs=10, batch_size=32)\n",
        "ctgan.fit(df, categoricals)\n",
        "\n",
        "# Generate the data\n",
        "synthetic = ctgan.sample(20000)\n",
        "synthetic.head()"
      ],
      "metadata": {
        "id": "5W4CXPZBUvTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split  \n",
        "\n",
        "X = synthetic.drop(['Category'], axis=1) #Not scaled\n",
        "y = synthetic['Category']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "s = StandardScaler()\n",
        "\n",
        "X_train = s.fit_transform(X_train)\n",
        "X_test = s.transform(X_test)"
      ],
      "metadata": {
        "id": "4wq8n8aVZZIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize a PCA object\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA object to the data\n",
        "pca.fit(X_train)\n",
        "\n",
        "# Create a scree plot\n",
        "num_components = len(pca.explained_variance_ratio_)\n",
        "plt.plot(np.arange(1, num_components+1), pca.explained_variance_ratio_, 'ro-', linewidth=2)\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Proportion of Variance Explained')\n",
        "plt.show()\n",
        "\n",
        "# Determine the number of components to keep\n",
        "variance_threshold = 0.95\n",
        "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "num_components_to_keep = np.argmax(cumulative_variance_ratio >= variance_threshold) + 1\n",
        "\n",
        "print(f'\\n Number of components to keep: {num_components_to_keep}')\n",
        "\n",
        "# Transform the data using the chosen number of components\n",
        "pca = PCA(n_components=num_components_to_keep)\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)"
      ],
      "metadata": {
        "id": "A8qupvDzZQc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the cumulative variance explained\n",
        "\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "cum = np.insert(cumulative_variance_ratio, 0, 0)\n",
        "ylab = np.insert(np.cumsum(pca.explained_variance_ratio_), 0, 0)\n",
        "\n",
        "plt.plot(cum, 'ro-', linewidth=2)\n",
        "plt.title('Cumulative Variance Explained')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Variance Explained')\n",
        "plt.yticks(ylab)\n",
        "plt.xticks(np.arange(0, 21))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dvybx8sUZp6u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}