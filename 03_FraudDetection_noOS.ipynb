{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNN9SCnSS2BSqsxXPT4S5+n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giakomorssi/Deloitte_Project/blob/main/03_FraudDetection_noOS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the Data"
      ],
      "metadata": {
        "id": "t0315A0JjvY-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 849,
      "metadata": {
        "id": "ffbsL3U8jkV5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb515afc-c32d-40d8-e848-cb78bcd8e01d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Runtime switched to GPU\n",
            "GPU device not found\n",
            "Please install GPU version of TF\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Change Colab runtime to GPU\n",
        "import os\n",
        "os.environ['COLAB_TPU_ADDR'] = ''\n",
        "os.environ['COLAB_GPU_ALLOC'] = '1'\n",
        "os.environ['COLAB_GPU'] = '1'\n",
        "print(\"Runtime switched to GPU\")\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "if not tf.test.gpu_device_name():\n",
        "    print('GPU device not found')\n",
        "else:\n",
        "    print('GPU device found:', tf.test.gpu_device_name())\n",
        "\n",
        "# This code sets the runtime to use the GPU if available\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"Please install GPU version of TF\")\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/University/Deloitte/SupplyChainDataset.csv', encoding = 'latin-1')"
      ],
      "metadata": {
        "id": "aboYVnjYjxh5"
      },
      "execution_count": 850,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning"
      ],
      "metadata": {
        "id": "lYRBHmf-j3_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Na and Empty Columns\n",
        "\n",
        "df.drop(['Product Description', 'Order Zipcode', 'Order Profit Per Order', 'Customer Email', 'Customer Password'], axis = 1, inplace = True) \n",
        "df.dropna(inplace = True) #remove 1 missing value"
      ],
      "metadata": {
        "id": "rTjitksTj5vi"
      },
      "execution_count": 851,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Category Column"
      ],
      "metadata": {
        "id": "N1wlvoXlj8Mu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **COMPLETE:** The order or transaction has been successfully fulfilled and completed.\n",
        "2. **PENDING**: The order or transaction is still in progress and has not yet been completed.\n",
        "3. **CLOSED**: The order or transaction has been closed or terminated for some reason, such as a return or cancellation.\n",
        "4. **PENDING_PAYMENT**: The order or transaction is awaiting payment before it can be processed.\n",
        "5. **CANCELED**: The order or transaction has been canceled by the customer or the seller for some reason.\n",
        "6. **PROCESSING**: The order or transaction is being processed by the seller or merchant.\n",
        "7. **SUSPECTED_FRAUD**: The order or transaction is under review due to suspected fraudulent activity.\n",
        "8. **ON_HOLD**: The order or transaction has been placed on hold for some reason, such as a delay in shipping or a credit hold.\n",
        "9. **PAYMENT_REVIEW**: The payment for the order or transaction is under review by the payment processor or financial institution."
      ],
      "metadata": {
        "id": "1P3--qOaj_Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regular -> Complete, Pending, Pending_Payment, Processing\n",
        "# Suspected -> Closed, Canceled, On_Hold, Payment_Review\n",
        "# Fraud -> Suspected_Fraud\n",
        "\n",
        "# define dictionaries to map status values to categories\n",
        "regular_dict = {'COMPLETE': 'Regular', 'PENDING': 'Regular', 'PENDING_PAYMENT': 'Regular', 'PROCESSING': 'Regular'}\n",
        "suspected_dict = {'CLOSED': 'Suspected', 'CANCELED': 'Suspected', 'ON_HOLD': 'Suspected', 'PAYMENT_REVIEW': 'Suspected'}\n",
        "fraud_dict = {'SUSPECTED_FRAUD': 'Fraud'}\n",
        "\n",
        "# create a function to map status values to categories\n",
        "def map_category(status):\n",
        "    if status in regular_dict:\n",
        "        return regular_dict[status]\n",
        "    elif status in suspected_dict:\n",
        "        return suspected_dict[status]\n",
        "    elif status in fraud_dict:\n",
        "        return fraud_dict[status]\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "# apply the function to the 'status' column to create a new 'category' column\n",
        "df['Category'] = df['Order Status'].apply(map_category)\n",
        "\n",
        "print('Regular: ', len([x for x in df['Category'] if x == 'Regular']), '\\n')\n",
        "print('Suspected: ', len([x for x in df['Category'] if x == 'Suspected']), '\\n')\n",
        "print('Fraud: ', len([x for x in df['Category'] if x == 'Fraud']))"
      ],
      "metadata": {
        "id": "PErUmIXSj-dg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb3b8f1-5fc8-4cd2-820f-89b17e211d44"
      },
      "execution_count": 852,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regular:  141442 \n",
            "\n",
            "Suspected:  35004 \n",
            "\n",
            "Fraud:  4062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objs as go\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "\n",
        "temp = df[\"Category\"].value_counts()\n",
        "df1 = pd.DataFrame({'Category': temp.index,'values': temp.values})\n",
        "\n",
        "# Define a list of colors for the bars\n",
        "colors = ['red', 'blue', 'green']\n",
        "\n",
        "traces = []\n",
        "for i, category in enumerate(df1['Category']):\n",
        "    if category == 1:\n",
        "        name = \"Regular\"\n",
        "    elif category == 0:\n",
        "        name = \"Suspected\"\n",
        "    else:\n",
        "        name = \"Fraud\"\n",
        "    trace = go.Bar(\n",
        "        x=[name], y=[df1.loc[i, 'values']],\n",
        "        name=name,\n",
        "        marker=dict(color=colors[i]),\n",
        "        text=[df1.loc[i, 'values']],\n",
        "        legendgroup=\"group\"\n",
        "    )\n",
        "    traces.append(trace)\n",
        "\n",
        "layout = dict(title='Credit Card Fraud Class - data unbalance',\n",
        "              xaxis=dict(title='Class', showticklabels=True), \n",
        "              yaxis=dict(title='Number of transactions'),\n",
        "              hovermode='closest', width=600,\n",
        "              showlegend=True\n",
        "             )\n",
        "fig = go.Figure(data=traces, layout=layout)\n",
        "iplot(fig, filename='class')"
      ],
      "metadata": {
        "id": "2jPOt1hs2CKN",
        "outputId": "2ff1f725-86b5-457f-f40d-52c449e7ca8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "execution_count": 853,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"6c84f1f2-240e-443b-8f71-84e355096365\" class=\"plotly-graph-div\" style=\"height:525px; width:600px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6c84f1f2-240e-443b-8f71-84e355096365\")) {                    Plotly.newPlot(                        \"6c84f1f2-240e-443b-8f71-84e355096365\",                        [{\"legendgroup\":\"group\",\"marker\":{\"color\":\"red\"},\"name\":\"Fraud\",\"text\":[\"141442\"],\"x\":[\"Fraud\"],\"y\":[141442],\"type\":\"bar\"},{\"legendgroup\":\"group\",\"marker\":{\"color\":\"blue\"},\"name\":\"Fraud\",\"text\":[\"35004\"],\"x\":[\"Fraud\"],\"y\":[35004],\"type\":\"bar\"},{\"legendgroup\":\"group\",\"marker\":{\"color\":\"green\"},\"name\":\"Fraud\",\"text\":[\"4062\"],\"x\":[\"Fraud\"],\"y\":[4062],\"type\":\"bar\"}],                        {\"hovermode\":\"closest\",\"showlegend\":true,\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Credit Card Fraud Class - data unbalance\"},\"width\":600,\"xaxis\":{\"showticklabels\":true,\"title\":{\"text\":\"Class\"}},\"yaxis\":{\"title\":{\"text\":\"Number of transactions\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6c84f1f2-240e-443b-8f71-84e355096365');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding"
      ],
      "metadata": {
        "id": "S_JKQJl9kC5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df.drop(['Category Name', 'Customer City',\n",
        "       'Customer Country', 'Customer Fname', 'Customer Id', 'Customer Lname',\n",
        "       'Customer State',\t'Customer Street', \n",
        "       'Order City', 'Order Country', 'Order Customer Id', 'Order Region',\t\n",
        "       'Order State', 'Product Image',\t'Product Name',\n",
        "       'shipping date (DateOrders)', 'order date (DateOrders)', 'Category Id', 'Customer Zipcode', \n",
        "       'Department Id', 'Latitude',\t'Longitude', 'Order Id',\t'Order Item Cardprod Id',\n",
        "       'Order Item Id', 'Product Card Id', 'Product Category Id'], axis = 1, inplace = True)\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/University/Deloitte/df_number.csv', index = False)"
      ],
      "metadata": {
        "id": "Zn7gWskckHxx"
      },
      "execution_count": 854,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split  \n",
        "\n",
        "df.drop(['Order Status'], axis=1, inplace=True)\n",
        "\n",
        "X = df.drop(['Category'], axis=1)\n",
        "y = df['Category']"
      ],
      "metadata": {
        "id": "eezGwCHSkNB9"
      },
      "execution_count": 855,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Type\n",
        "onehot = pd.get_dummies(X['Type'])\n",
        "X = pd.concat([X, onehot], axis=1)\n",
        "X.drop(['Type'], axis=1, inplace=True)\n",
        "\n",
        "# Department Name\n",
        "onehot = pd.get_dummies(X['Department Name'])\n",
        "X = pd.concat([X, onehot], axis=1)\n",
        "X.drop(['Department Name'], axis=1, inplace=True)\n",
        "\n",
        "# Market\n",
        "onehot = pd.get_dummies(X['Market'])\n",
        "X = pd.concat([X, onehot], axis=1)\n",
        "X.drop(['Market'], axis=1, inplace=True)\n",
        "\n",
        "# Customer Segment\n",
        "onehot = pd.get_dummies(X['Customer Segment'])\n",
        "X = pd.concat([X, onehot], axis=1)\n",
        "X.drop(['Customer Segment'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "pQrkuJkenwf0"
      },
      "execution_count": 856,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True)"
      ],
      "metadata": {
        "id": "4KK51NNrqhm8"
      },
      "execution_count": 857,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Shipping Mode\n",
        "custom_order = ['Same Day', 'First Class', 'Second Class', 'Standard Class']\n",
        "le.fit(custom_order)\n",
        "X_train['Shipping Mode'] = le.fit_transform(X_train['Shipping Mode'])\n",
        "X_test['Shipping Mode'] = le.transform(X_test['Shipping Mode'])\n",
        "\n",
        "# Delivery Status\n",
        "# Define the custom order\n",
        "custom_order = ['Shipping on time', 'Advance shipping', 'Late delivery', 'Shipping canceled']\n",
        "le.fit(custom_order)\n",
        "X_train['Delivery Status'] = le.fit_transform(X_train['Delivery Status'])\n",
        "X_test['Delivery Status'] = le.transform(X_test['Delivery Status'])\n",
        "\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)"
      ],
      "metadata": {
        "id": "qaz4tUWZkFKd"
      },
      "execution_count": 858,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define the column transformer to apply scaling only to non-one-hot-encoded columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), list(range(0, 16)))\n",
        "    ],\n",
        "    remainder=\"passthrough\"\n",
        ")\n",
        "\n",
        "column_names = list(X_train.columns)\n",
        "\n",
        "# Create a pipeline with the preprocessor and any other desired transformers\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    # add other transformers or classifiers as desired\n",
        "])\n",
        "\n",
        "# Fit and transform the input data using the pipeline\n",
        "X_train = pipeline.fit_transform(X_train)\n",
        "X_test = pipeline.transform(X_test)\n",
        "\n",
        "X_train = pd.DataFrame(X_train, columns=column_names)\n",
        "X_test = pd.DataFrame(X_test, columns=column_names)"
      ],
      "metadata": {
        "id": "o6j623udkPNc"
      },
      "execution_count": 859,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X_train = pd.DataFrame(X_train)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "y_train = np.ravel(y_train)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(type(X), type(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZKOgCbeELOn",
        "outputId": "3aa1b25e-3fef-46ac-bde3-6ca98e5b2c0f"
      },
      "execution_count": 860,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(144406, 39) (144406,)\n",
            "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.series.Series'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA"
      ],
      "metadata": {
        "id": "tB47s_04FOjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize a PCA object\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA object to the data\n",
        "pca.fit(X_train.iloc[:, :16])\n",
        "\n",
        "# Determine the number of components to keep\n",
        "variance_threshold = 0.95\n",
        "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "num_components_to_keep = np.argmax(cumulative_variance_ratio >= variance_threshold) + 1\n",
        "\n",
        "print(f'\\n Number of components to keep: {num_components_to_keep}')\n",
        "\n",
        "# Transform the data using the chosen number of components\n",
        "pca = PCA(n_components=num_components_to_keep)\n",
        "pca_train = pca.fit_transform(X_train.iloc[:, :16])\n",
        "pca_test = pca.transform(X_test.iloc[:, :16])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "au3huJShFQyl",
        "outputId": "8edfed5c-c08f-4014-b5a8-4666efc130eb"
      },
      "execution_count": 861,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Number of components to keep: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_data_train = X_train.iloc[:, 16:]\n",
        "onehot_data_test = X_test.iloc[:, 16:]"
      ],
      "metadata": {
        "id": "7iSefjj0ZhDo"
      },
      "execution_count": 862,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.concat([pd.DataFrame(pca_train), pd.DataFrame(onehot_data_train)], axis=1)\n",
        "X_test = pd.concat([pd.DataFrame(pca_test), pd.DataFrame(onehot_data_test)], axis=1)"
      ],
      "metadata": {
        "id": "XSKk2D33MIxY"
      },
      "execution_count": 863,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = pd.DataFrame(X_test)"
      ],
      "metadata": {
        "id": "yydLIv1hsqtN"
      },
      "execution_count": 864,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)"
      ],
      "metadata": {
        "id": "svdl37l9soS2"
      },
      "execution_count": 865,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "P-NJnauSkf84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(class_weight='balanced', n_estimators=100, max_depth=4)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "with open('rf.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "0i45eqSSh_QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score\n",
        "import pickle\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)\n",
        "\n",
        "with open('rf.pkl', 'rb') as file:\n",
        "    model = pickle.load(file)\n",
        "\n",
        "print(model)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print('Confusion Matrix\\n', confusion_matrix(y_test, model.predict(X_test)), '\\n')\n",
        "print('Recalls', recall_score(y_test, y_pred, average=None))\n",
        "print('Precisions', precision_score(y_test, y_pred, average=None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VEPI75g2_Ly",
        "outputId": "8c74d13d-ac11-4b2d-e7b6-68a9241a59bc"
      },
      "execution_count": 866,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier(class_weight='balanced', max_depth=4)\n",
            "Confusion Matrix\n",
            " [[  807     5     0]\n",
            " [ 2264 26025     0]\n",
            " [  754  2334  3913]] \n",
            "\n",
            "Recalls [0.9938 0.92   0.5589]\n",
            "Precisions [0.211  0.9175 1.    ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "model = GradientBoostingClassifier(loss='exponential', learning_rate=0.1, n_estimators=100, max_depth=4)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "with open('gb.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "hryRW5TriAWD"
      },
      "execution_count": 803,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score\n",
        "import pickle\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)\n",
        "\n",
        "with open('gb.pkl', 'rb') as file:\n",
        "    model = pickle.load(file)\n",
        "\n",
        "print(model)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print('Confusion Matrix\\n', confusion_matrix(y_test, model.predict(X_test)), '\\n')\n",
        "print('Recalls', recall_score(y_test, y_pred, average=None))\n",
        "print('Precisions', precision_score(y_test, y_pred, average=None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFMOaDLM3AgL",
        "outputId": "4b76670a-4224-4d21-c764-eaa930e8823a"
      },
      "execution_count": 867,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GradientBoostingClassifier(max_depth=4)\n",
            "Confusion Matrix\n",
            " [[  399   157   256]\n",
            " [  199 26480  1610]\n",
            " [  314  2293  4394]] \n",
            "\n",
            "Recalls [0.4914 0.9361 0.6276]\n",
            "Precisions [0.4375 0.9153 0.7019]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model_bagging = BaggingClassifier(base_estimator=RandomForestClassifier(class_weight='balanced', n_estimators=100, max_depth=4), n_estimators=100)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "with open('bagging.pkl', 'wb') as file:\n",
        "    pickle.dump(model_bagging, file)"
      ],
      "metadata": {
        "id": "ZbGjta6PiAGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier, VotingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "model_voting = VotingClassifier(estimators=[], voting='hard')\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "with open('voting.pkl', 'wb') as file:\n",
        "    pickle.dump(model_voting, file)"
      ],
      "metadata": {
        "id": "Fpn_6LWQi2o-"
      },
      "execution_count": 826,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "model = xgb.XGBClassifier(objective='multi:softmax', num_class=3, n_estimators=200, learning_rate=0.1)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "with open('xgb.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "4f-P2hvxiJZA"
      },
      "execution_count": 687,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score\n",
        "import pickle\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)\n",
        "\n",
        "with open('xgb.pkl', 'rb') as file:\n",
        "    model = pickle.load(file)\n",
        "\n",
        "print(model)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print('Confusion Matrix\\n', confusion_matrix(y_test, model.predict(X_test)), '\\n')\n",
        "print('Recalls', recall_score(y_test, y_pred, average=None))\n",
        "print('Precisions', precision_score(y_test, y_pred, average=None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjnSiUrn3FXz",
        "outputId": "3a120f81-676f-450e-dba9-63938c6d10ca"
      },
      "execution_count": 868,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=None, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
            "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
            "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
            "              n_estimators=200, n_jobs=None, num_class=3,\n",
            "              num_parallel_tree=None, objective='multi:softmax', ...)\n",
            "Confusion Matrix\n",
            " [[  541    18   253]\n",
            " [    0 28245    44]\n",
            " [  380  2339  4282]] \n",
            "\n",
            "Recalls [0.6663 0.9984 0.6116]\n",
            "Precisions [0.5874 0.923  0.9351]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = AdaBoostClassifier(base_estimator=RandomForestClassifier(class_weight='balanced', n_estimators=100, max_depth=4), n_estimators=100)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "with open('ada.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWxnQH0RiJVA",
        "outputId": "99126a22-a5c7-49c4-e8c2-5754e3b88d17"
      },
      "execution_count": 873,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning:\n",
            "\n",
            "`base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score\n",
        "import pickle\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)\n",
        "\n",
        "with open('ada.pkl', 'rb') as file:\n",
        "    model = pickle.load(file)\n",
        "\n",
        "print(model)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print('Confusion Matrix\\n', confusion_matrix(y_test, model.predict(X_test)), '\\n')\n",
        "print('Recalls', recall_score(y_test, y_pred, average=None))\n",
        "print('Precisions', precision_score(y_test, y_pred, average=None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zms-jhkG3GvW",
        "outputId": "79860f0d-1edc-4b03-cca0-fde952251c4c"
      },
      "execution_count": 874,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoostClassifier(base_estimator=RandomForestClassifier(class_weight='balanced',\n",
            "                                                         max_depth=4),\n",
            "                   n_estimators=100)\n",
            "Confusion Matrix\n",
            " [[  795     0    17]\n",
            " [    0 27455   834]\n",
            " [  741  2228  4032]] \n",
            "\n",
            "Recalls [0.9791 0.9705 0.5759]\n",
            "Precisions [0.5176 0.9249 0.8257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "\n",
        "model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=3)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "with open('cat.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "hF9Q_rtBizd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score\n",
        "import pickle\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)\n",
        "\n",
        "with open('cat.pkl', 'rb') as file:\n",
        "    model = pickle.load(file)\n",
        "\n",
        "print(model)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print('Confusion Matrix\\n', confusion_matrix(y_test, model.predict(X_test)), '\\n')\n",
        "print('Recalls', recall_score(y_test, y_pred, average=None))\n",
        "print('Precisions', precision_score(y_test, y_pred, average=None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzhlOpnD3IWp",
        "outputId": "9745ebf6-308e-4ee6-b249-f62e7e754b78"
      },
      "execution_count": 829,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<catboost.core.CatBoostClassifier object at 0x7fb568b3b760>\n",
            "Confusion Matrix\n",
            " [[  585   219     8]\n",
            " [    0 28287     2]\n",
            " [  534  2519  3948]] \n",
            "\n",
            "Recalls [0.7204 0.9999 0.5639]\n",
            "Precisions [0.5228 0.9117 0.9975]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "model = GaussianMixture(n_components=3, covariance_type='full', max_iter=100, random_state=0)\n",
        "\n",
        "model.fit(X_train)\n",
        "\n",
        "with open('gmm.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "UTEIctlGigLE"
      },
      "execution_count": 691,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score\n",
        "import pickle\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)\n",
        "\n",
        "with open('gmm.pkl', 'rb') as file:\n",
        "    model = pickle.load(file)\n",
        "\n",
        "print(model)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print('Confusion Matrix\\n', confusion_matrix(y_test, model.predict(X_test)), '\\n')\n",
        "print('Recalls', recall_score(y_test, y_pred, average=None))\n",
        "print('Precisions', precision_score(y_test, y_pred, average=None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EazA23rI3N8S",
        "outputId": "57dafd22-7823-483a-8630-25536ae71f73"
      },
      "execution_count": 832,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GaussianMixture(n_components=3, random_state=0)\n",
            "Confusion Matrix\n",
            " [[  618   149    45]\n",
            " [19653  7169  1467]\n",
            " [ 5013  1644   344]] \n",
            "\n",
            "Recalls [0.7611 0.2534 0.0491]\n",
            "Precisions [0.0244 0.7999 0.1853]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "with open('lr.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "gcxN2ZYOY85N"
      },
      "execution_count": 693,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score\n",
        "import pickle\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)\n",
        "\n",
        "with open('lr.pkl', 'rb') as file:\n",
        "    model = pickle.load(file)\n",
        "\n",
        "print(model)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print('Confusion Matrix\\n', confusion_matrix(y_test, model.predict(X_test)), '\\n')\n",
        "print('Recalls', recall_score(y_test, y_pred, average=None))\n",
        "print('Precisions', precision_score(y_test, y_pred, average=None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcMV3UvV3SIG",
        "outputId": "c2512827-4a4f-45f1-8808-dbe8e1868c6e"
      },
      "execution_count": 834,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression(class_weight='balanced', max_iter=1000)\n",
            "Confusion Matrix\n",
            " [[  706   106     0]\n",
            " [ 2030 24432  1827]\n",
            " [  626  2118  4257]] \n",
            "\n",
            "Recalls [0.8695 0.8637 0.6081]\n",
            "Precisions [0.21   0.9166 0.6997]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = DecisionTreeClassifier(class_weight='balanced', max_depth = 4)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "with open('dt.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "12mfV1V4Y8mG"
      },
      "execution_count": 694,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score\n",
        "import pickle\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)\n",
        "\n",
        "with open('dt.pkl', 'rb') as file:\n",
        "    model = pickle.load(file)\n",
        "\n",
        "print(model)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print('Confusion Matrix\\n', confusion_matrix(y_test, model.predict(X_test)), '\\n')\n",
        "print('Recalls', recall_score(y_test, y_pred, average=None))\n",
        "print('Precisions', precision_score(y_test, y_pred, average=None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogRMFF1g3T5U",
        "outputId": "1520a5d7-b747-475e-b1c1-b1292c1a086a"
      },
      "execution_count": 835,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTreeClassifier(class_weight='balanced', max_depth=4)\n",
            "Confusion Matrix\n",
            " [[  812     0     0]\n",
            " [  264 28025     0]\n",
            " [  740  2320  3941]] \n",
            "\n",
            "Recalls [1.     0.9907 0.5629]\n",
            "Precisions [0.4471 0.9235 1.    ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "OCrPFvJokpeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score\n",
        "import pickle\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)\n",
        "\n",
        "with open('gb.pkl', 'rb') as file:\n",
        "    model = pickle.load(file)\n",
        "\n",
        "print(model)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print('Confusion Matrix\\n', confusion_matrix(y_test, model.predict(X_test)), '\\n')\n",
        "print('Recalls', recall_score(y_test, y_pred, average=None))\n",
        "print('Precisions', precision_score(y_test, y_pred, average=None))"
      ],
      "metadata": {
        "id": "a1uImn8w_aBW",
        "outputId": "cb091ace-94e2-4397-8f36-ccbf6a9cdf2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 664,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GradientBoostingClassifier(loss='deviance', max_depth=4)\n",
            "Confusion Matrix\n",
            " [[  475   131   206]\n",
            " [   25 28159   105]\n",
            " [  423  2452  4126]] \n",
            "\n",
            "Recalls [0.585  0.9954 0.5893]\n",
            "Precisions [0.5146 0.916  0.9299]\n",
            "Accuracy 0.9074289513046369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix, recall_score\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Standardize the data and split it into training and test sets\n",
        "s = StandardScaler()\n",
        "le = LabelEncoder()\n",
        "\n",
        "recall_scores = []\n",
        "precision_scores = []\n",
        "fraud_recall = []\n",
        "suspected_recall = []\n",
        "regular_recall = [] \n",
        "low = []\n",
        "avg_conf_matrix = np.zeros((3, 3))\n",
        "print(model)\n",
        "\n",
        "for i in range(1, 11):\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True)\n",
        "\n",
        "  # Shipping Mode\n",
        "  custom_order = ['Same Day', 'First Class', 'Second Class', 'Standard Class']\n",
        "  le.fit(custom_order)\n",
        "  X_train['Shipping Mode'] = le.fit_transform(X_train['Shipping Mode'])\n",
        "  X_test['Shipping Mode'] = le.transform(X_test['Shipping Mode'])\n",
        "\n",
        "  # Delivery Status\n",
        "  # Define the custom order\n",
        "  custom_order = ['Shipping on time', 'Advance shipping', 'Late delivery', 'Shipping canceled']\n",
        "  le.fit(custom_order)\n",
        "  X_train['Delivery Status'] = le.fit_transform(X_train['Delivery Status'])\n",
        "  X_test['Delivery Status'] = le.transform(X_test['Delivery Status'])\n",
        "\n",
        "  # Category\n",
        "  y_train = le.fit_transform(y_train)\n",
        "  y_test = le.transform(y_test)\n",
        "\n",
        "  preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), list(range(0, 16)))\n",
        "    ],\n",
        "    remainder=\"passthrough\")\n",
        "\n",
        "  pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor)])\n",
        "\n",
        "  X_train = pipeline.fit_transform(X_train)\n",
        "  X_test = pipeline.transform(X_test)\n",
        "\n",
        "\n",
        "  # PCA\n",
        "  pca = PCA()\n",
        "\n",
        "  # Fit the PCA object to the data\n",
        "  X_train = pd.DataFrame(X_train)\n",
        "  X_test = pd.DataFrame(X_test)\n",
        "  pca.fit(X_train.iloc[:, :16])\n",
        "\n",
        "  # Determine the number of components to keep\n",
        "  variance_threshold = 0.95\n",
        "  cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "  num_components_to_keep = np.argmax(cumulative_variance_ratio >= variance_threshold) + 1\n",
        "\n",
        "  # Transform the data using the chosen number of components\n",
        "  pca = PCA(n_components=num_components_to_keep)\n",
        "  pca_train = pca.fit_transform(X_train.iloc[:, :16])\n",
        "  pca_test = pca.transform(X_test.iloc[:, :16])\n",
        "\n",
        "  onehot_data_train = X_train.iloc[:, 16:]\n",
        "  onehot_data_test = X_test.iloc[:, 16:]\n",
        "\n",
        "  X_train = pd.concat([pd.DataFrame(pca_train), pd.DataFrame(onehot_data_train)], axis=1)\n",
        "  X_test = pd.concat([pd.DataFrame(pca_test), pd.DataFrame(onehot_data_test)], axis=1)\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  recall_scores.append(recall_score(y_test, y_pred, average=None))\n",
        "  fraud_recall.append(recall_score(y_test, y_pred, average=None)[0])\n",
        "  regular_recall.append(recall_score(y_test, y_pred, average=None)[1])\n",
        "  suspected_recall.append(recall_score(y_test, y_pred, average=None)[2])\n",
        "  precision_scores.append(precision_score(y_test, y_pred, average=None))\n",
        "\n",
        "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "  avg_conf_matrix += conf_matrix\n",
        "\n",
        "  if i % 10 == 0:\n",
        "      print(f'Iteration: {i}')\n",
        "      print(f'Fraud Recall {round(np.average(fraud_recall), 4)}, {round(np.std(fraud_recall), 4)}')\n",
        "      print(f'Suspected Recall {round(np.average(suspected_recall), 4)}, {round(np.std(suspected_recall), 4)}')\n",
        "      print(f'Regular Recall {round(np.average(regular_recall), 4)}, {round(np.std(regular_recall), 4)}')\n",
        "      print(f'Total Recall {round(np.average(recall_scores), 4)}, {round(np.std(recall_scores), 4)} \\n')\n",
        "\n",
        "  if recall_score(y_test, y_pred, average=None)[0] < 0.7:\n",
        "    low.append(round(recall_score(y_test, y_pred, average=None)[0], 4))\n",
        "  \n",
        "print(f'\\n Fraud Recall: {round(np.average(fraud_recall), 4)}, std: {round(np.std(fraud_recall), 4)}, Under 0.7: {len(low)}, {low}\\n Suspected Recall: {round(np.average(suspected_recall), 4)}, std: {round(np.std(suspected_recall), 4)}\\n Regular Recall: {round(np.average(regular_recall), 4)}, std: {round(np.std(regular_recall), 4)}\\n Total: {round(np.average(recall_scores), 4)}, std: {round(np.std(recall_scores), 4)}')\n",
        "\n",
        "np.set_printoptions(precision=4)\n",
        "avg_conf_matrix /= 20\n",
        "print(\"\\n Average Confusion Matrix:\")\n",
        "print(avg_conf_matrix)\n",
        "\n",
        "print('Precisions', precision_score(y_test, y_pred, average=None))\n",
        "\n",
        "print('\\n', model)"
      ],
      "metadata": {
        "id": "YWk3Dt0hkrJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synthetic Data"
      ],
      "metadata": {
        "id": "plqxS6GvUrRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.replace({0: 'Fraud', 1: 'Regular', 2: 'Suspected'}, inplace=True)"
      ],
      "metadata": {
        "id": "y6C_lswQdeI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X_test.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)\n",
        "test = pd.concat([X_test, y_test], axis=1)\n",
        "test"
      ],
      "metadata": {
        "id": "k0B2eUeocloa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/sdv-dev/CTGAN.git"
      ],
      "metadata": {
        "id": "cf58lz7WhBJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.select_dtypes(exclude=\"number\").columns.tolist()"
      ],
      "metadata": {
        "id": "iXOpyc4Kl4IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4ZaYzbIUl8T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ctgan import CTGAN\n",
        "import pandas as pd\n",
        "\n",
        "categoricals = test.select_dtypes(exclude=\"number\").columns.tolist()\n",
        "\n",
        "\n",
        "ctgan = CTGAN(epochs=10)\n",
        "ctgan.fit(test[:-1], categoricals)\n",
        "\n",
        "synthetic_data = ctgan.sample(len(test))"
      ],
      "metadata": {
        "id": "7ioy7LJygxFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "veIJs4BOluYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from ctgan import CTGAN\n",
        "\n",
        "categoricals = df.select_dtypes(exclude=\"number\").columns.tolist()\n",
        "\n",
        "# Fit CTGAN\n",
        "ctgan = CTGAN(epochs=10, batch_size=32)\n",
        "ctgan.fit(df, categoricals)\n",
        "\n",
        "# Generate the data\n",
        "synthetic = ctgan.sample(20000)\n",
        "synthetic.head()"
      ],
      "metadata": {
        "id": "5W4CXPZBUvTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split  \n",
        "\n",
        "X = synthetic.drop(['Category'], axis=1) #Not scaled\n",
        "y = synthetic['Category']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "s = StandardScaler()\n",
        "\n",
        "X_train = s.fit_transform(X_train)\n",
        "X_test = s.transform(X_test)"
      ],
      "metadata": {
        "id": "4wq8n8aVZZIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize a PCA object\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA object to the data\n",
        "pca.fit(X_train)\n",
        "\n",
        "# Create a scree plot\n",
        "num_components = len(pca.explained_variance_ratio_)\n",
        "plt.plot(np.arange(1, num_components+1), pca.explained_variance_ratio_, 'ro-', linewidth=2)\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Proportion of Variance Explained')\n",
        "plt.show()\n",
        "\n",
        "# Determine the number of components to keep\n",
        "variance_threshold = 0.95\n",
        "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "num_components_to_keep = np.argmax(cumulative_variance_ratio >= variance_threshold) + 1\n",
        "\n",
        "print(f'\\n Number of components to keep: {num_components_to_keep}')\n",
        "\n",
        "# Transform the data using the chosen number of components\n",
        "pca = PCA(n_components=num_components_to_keep)\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)"
      ],
      "metadata": {
        "id": "A8qupvDzZQc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the cumulative variance explained\n",
        "\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "cum = np.insert(cumulative_variance_ratio, 0, 0)\n",
        "ylab = np.insert(np.cumsum(pca.explained_variance_ratio_), 0, 0)\n",
        "\n",
        "plt.plot(cum, 'ro-', linewidth=2)\n",
        "plt.title('Cumulative Variance Explained')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Variance Explained')\n",
        "plt.yticks(ylab)\n",
        "plt.xticks(np.arange(0, 21))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dvybx8sUZp6u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}