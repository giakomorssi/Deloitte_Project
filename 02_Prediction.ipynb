{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giakomorssi/Deloitte_Project/blob/main/02_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orgvMZprijUi"
      },
      "source": [
        "# Import the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YbdOFcuimXI",
        "outputId": "785617f5-1bdd-44a0-8a5b-4499dd1b1390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Runtime switched to GPU\n",
            "GPU device found: /device:GPU:0\n",
            "Default GPU Device: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Change Colab runtime to GPU\n",
        "import os\n",
        "os.environ['COLAB_TPU_ADDR'] = ''\n",
        "os.environ['COLAB_GPU_ALLOC'] = '1'\n",
        "os.environ['COLAB_GPU'] = '1'\n",
        "print(\"Runtime switched to GPU\")\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "if not tf.test.gpu_device_name():\n",
        "    print('GPU device not found')\n",
        "else:\n",
        "    print('GPU device found:', tf.test.gpu_device_name())\n",
        "\n",
        "# This code sets the runtime to use the GPU if available\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"Please install GPU version of TF\")\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "!pip install -q category_encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKG4GTnhisz4"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/University/Deloitte/SupplyChainDataset.csv', encoding = 'latin-1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWw2dp9MeOTe"
      },
      "source": [
        "# Cleaning the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzGJHtMaeF0i"
      },
      "outputs": [],
      "source": [
        "# Remove Na and Empty Columns\n",
        "\n",
        "df.drop(['Product Description', 'Order Zipcode', 'Order Profit Per Order', 'Customer Email', 'Customer Password', 'Customer Country', 'Customer Id', 'Customer Fname', 'Customer Lname', 'Customer Street', 'Order Country', 'Product Card Id', 'Product Category Id', 'Product Image', 'Customer State', 'shipping date (DateOrders)', 'order date (DateOrders)'], axis = 1, inplace = True) \n",
        "df.dropna(inplace = True) #remove 1 missing value"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('/content/drive/MyDrive/University/Deloitte/df_lr.csv', index = False)"
      ],
      "metadata": {
        "id": "h5mhshDUPuUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbisUTEaeX1J"
      },
      "source": [
        "## Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnVQgjIrefSP"
      },
      "source": [
        "### Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHMxOu0xevti"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop(['Days for shipping (real)', 'Product Name'], axis = 1)\n",
        "y = df['Days for shipping (real)']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDM9u2iYib2M"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOBfHPEphRx7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from category_encoders import LeaveOneOutEncoder\n",
        "\n",
        "# initialize the encoder\n",
        "enc = LeaveOneOutEncoder(cols=['Customer City', 'Order City', 'Order State', 'Order Region'])\n",
        "\n",
        "# fit and transform the entire dataset\n",
        "X_train = enc.fit_transform(X_train, y_train)\n",
        "X_test = enc.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Select columns for one-hot encoding\n",
        "one_hot_cols = [0, 7, 9, 12, 15, 30]\n",
        "# Type, Department Name, Category Name, Market, Order Status, Customer Segment\n",
        "\n",
        "# Fit one-hot encoder to training data\n",
        "one_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "\n",
        "# Apply one-hot encoder to training and test data\n",
        "X_train_one_hot = one_hot_encoder.fit_transform(X_train.iloc[:, one_hot_cols])\n",
        "X_test_one_hot = one_hot_encoder.transform(X_test.iloc[:, one_hot_cols])\n",
        "\n",
        "# Remove original columns from training and test data\n",
        "X_train = X_train.drop(X_train.columns[one_hot_cols], axis=1)\n",
        "X_test = X_test.drop(X_test.columns[one_hot_cols], axis=1)\n",
        "\n",
        "# Concatenate one-hot encoded columns with remaining data\n",
        "X_train = pd.concat([pd.DataFrame(X_train_one_hot.toarray()), X_train.reset_index(drop=True)], axis=1)\n",
        "X_test = pd.concat([pd.DataFrame(X_test_one_hot.toarray()), X_test.reset_index(drop=True)], axis=1)"
      ],
      "metadata": {
        "id": "Wxy0wa1wsKpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly4l3rMgedcK"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Shipping Mode\n",
        "custom_order = ['Same Day', 'First Class', 'Second Class', 'Standard Class']\n",
        "le.fit(custom_order)\n",
        "X_train['Shipping Mode'] = le.fit_transform(X_train['Shipping Mode'])\n",
        "X_test['Shipping Mode'] = le.transform(X_test['Shipping Mode'])\n",
        "\n",
        "# Delivery Status\n",
        "# Define the custom order\n",
        "custom_order = ['Shipping on time', 'Advance shipping', 'Late delivery', 'Shipping canceled']\n",
        "le.fit(custom_order)\n",
        "X_train['Delivery Status'] = le.fit_transform(X_train['Delivery Status'])\n",
        "X_test['Delivery Status'] = le.transform(X_test['Delivery Status'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKD99kAWo78q"
      },
      "source": [
        "## Scale the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3_PeKvho9h7",
        "outputId": "98638369-c856-4c4a-9b63-ac8e925155ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-60-6700864cfc85>:5: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  X_train.iloc[:, 82:] = scaler.fit_transform(X_train.iloc[:, 82:])\n",
            "<ipython-input-60-6700864cfc85>:6: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  X_test.iloc[:, 82:] = scaler.transform(X_test.iloc[:, 82:])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train.iloc[:, 82:] = scaler.fit_transform(X_train.iloc[:, 82:])\n",
        "X_test.iloc[:, 82:] = scaler.transform(X_test.iloc[:, 82:])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = pd.DataFrame(X_test)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "y_train = np.ravel(y_train)\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)"
      ],
      "metadata": {
        "id": "ocJmtCbxu7UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFjf5jaOe6g7"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQxfil1yfMci"
      },
      "source": [
        "## Choosing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjkw8naNfIlZ",
        "outputId": "1359364f-25bd-485a-d547-a768fa11dba4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7503090876504547"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import pickle\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, max_depth=None)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "with open('/content/drive/MyDrive/University/Deloitte/models_lr/rf.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "mean_squared_error(y_test, model.predict(X_test), squared=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05kRLz35y1BN",
        "outputId": "e7ccbf9d-21ac-41df-945c-eb161f21ae44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8384870591728664"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "model = XGBRegressor(n_estimators=100, max_depth=None)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "with open('/content/drive/MyDrive/University/Deloitte/models_lr/xgb.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "mean_squared_error(y_test, model.predict(X_test), squared=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgvD4php86f_"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras import regularizers\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Define the neural network architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(516, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))) \n",
        "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model with mean squared error loss function and Adam optimizer\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
        "\n",
        "# Fit the model to the training data\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, mse = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Mean Squared Error on test set: {:.2f}'.format(mse))\n",
        "\n",
        "# Plot the training and validation loss over epochs\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'], label='training loss')\n",
        "plt.plot(history.history['val_loss'], label='validation loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aArVY7bvziW-",
        "outputId": "ef229044-1537-4106-9af0-fcc067f7dd9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.639807823600399"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# gradiant boosting\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "model = GradientBoostingRegressor(n_estimators=100)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "with open('/content/drive/MyDrive/University/Deloitte/models_lr/gb.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "mean_squared_error(y_test, model.predict(X_test), squared=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1F5sebp2H6T",
        "outputId": "6f9a0a90-88b9-4a6b-e0c7-40b48e3f2084"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.802427350097214"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Fit the model using QuantReg\n",
        "quant_model = sm.QuantReg(y_train, X_train)\n",
        "\n",
        "# Set the tau value to 0.5, which corresponds to median regression\n",
        "quant_result = quant_model.fit(q=0.7, max_iter=2000)\n",
        "\n",
        "# Print the summary of the model\n",
        "#print(quant_result.summary())\n",
        "\n",
        "# Make predictions on new data\n",
        "new_data = X_test\n",
        "predictions = quant_result.predict(new_data)\n",
        "\n",
        "mean_squared_error(y_test, predictions, squared=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "orgvMZprijUi",
        "eWw2dp9MeOTe",
        "VQxfil1yfMci"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyMQ0fyMY16OpgcqISKrwHPs",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}