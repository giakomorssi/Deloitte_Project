{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDz4VdR8dlW6xgJfYUKXIZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giakomorssi/Deloitte_Project/blob/main/03_FraudDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the Data"
      ],
      "metadata": {
        "id": "xkZYyo9diPRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Change Colab runtime to GPU\n",
        "import os\n",
        "os.environ['COLAB_TPU_ADDR'] = ''\n",
        "os.environ['COLAB_GPU_ALLOC'] = '1'\n",
        "os.environ['COLAB_GPU'] = '1'\n",
        "print(\"Runtime switched to GPU\")\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "if not tf.test.gpu_device_name():\n",
        "    print('GPU device not found')\n",
        "else:\n",
        "    print('GPU device found:', tf.test.gpu_device_name())\n",
        "\n",
        "# This code sets the runtime to use the GPU if available\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"Please install GPU version of TF\")\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "PO9N_ZOqiRnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/University/Deloitte/SupplyChainDataset.csv', encoding = 'latin-1')"
      ],
      "metadata": {
        "id": "Suold97ziTar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning"
      ],
      "metadata": {
        "id": "0Jf1zK8UgXMt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9Io1pqWgJOm"
      },
      "outputs": [],
      "source": [
        "# Remove Na and Empty Columns\n",
        "\n",
        "df.drop(['Product Description', 'Order Zipcode', 'Order Profit Per Order', 'Customer Email', 'Customer Password'], axis = 1, inplace = True) \n",
        "df.dropna(inplace = True) #remove 1 missing value"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Category Column\n",
        "\n",
        "1. **COMPLETE:** The order or transaction has been successfully fulfilled and completed.\n",
        "2. **PENDING**: The order or transaction is still in progress and has not yet been completed.\n",
        "3. **CLOSED**: The order or transaction has been closed or terminated for some reason, such as a return or cancellation.\n",
        "4. **PENDING_PAYMENT**: The order or transaction is awaiting payment before it can be processed.\n",
        "5. **CANCELED**: The order or transaction has been canceled by the customer or the seller for some reason.\n",
        "6. **PROCESSING**: The order or transaction is being processed by the seller or merchant.\n",
        "7. **SUSPECTED_FRAUD**: The order or transaction is under review due to suspected fraudulent activity.\n",
        "8. **ON_HOLD**: The order or transaction has been placed on hold for some reason, such as a delay in shipping or a credit hold.\n",
        "9. **PAYMENT_REVIEW**: The payment for the order or transaction is under review by the payment processor or financial institution."
      ],
      "metadata": {
        "id": "Rlrzv05Kgdel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regular -> Complete, Pending, Pending_Payment, Processing\n",
        "# Suspected -> Closed, Canceled, On_Hold, Payment_Review\n",
        "# Fraud -> Suspected_Fraud\n",
        "\n",
        "# define dictionaries to map status values to categories\n",
        "regular_dict = {'COMPLETE': 'Regular', 'PENDING': 'Regular', 'PENDING_PAYMENT': 'Regular', 'PROCESSING': 'Regular'}\n",
        "suspected_dict = {'CLOSED': 'Suspected', 'CANCELED': 'Suspected', 'ON_HOLD': 'Suspected', 'PAYMENT_REVIEW': 'Suspected'}\n",
        "fraud_dict = {'SUSPECTED_FRAUD': 'Fraud'}\n",
        "\n",
        "# create a function to map status values to categories\n",
        "def map_category(status):\n",
        "    if status in regular_dict:\n",
        "        return regular_dict[status]\n",
        "    elif status in suspected_dict:\n",
        "        return suspected_dict[status]\n",
        "    elif status in fraud_dict:\n",
        "        return fraud_dict[status]\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "# apply the function to the 'status' column to create a new 'category' column\n",
        "df['Category'] = df['Order Status'].apply(map_category)\n",
        "\n",
        "print('Regular: ', len([x for x in df['Category'] if x == 'Regular']), '\\n')\n",
        "print('Suspected: ', len([x for x in df['Category'] if x == 'Suspected']), '\\n')\n",
        "print('Fraud: ', len([x for x in df['Category'] if x == 'Fraud']))"
      ],
      "metadata": {
        "id": "FxgQetrnggaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding"
      ],
      "metadata": {
        "id": "PbZbT7PNgl2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Type\n",
        "df['Type'] = le.fit_transform(df['Type'])\n",
        "\n",
        "# Delivery Status\n",
        "df['Delivery Status'] = le.fit_transform(df['Delivery Status'])\n",
        "\n",
        "# Customer Segment\n",
        "df['Customer Segment'] = le.fit_transform(df['Customer Segment'])\n",
        "\n",
        "# Order Status\n",
        "df['Order Status'] = le.fit_transform(df['Order Status'])\n",
        "\n",
        "# Shipping Mode\n",
        "df['Shipping Mode'] = le.fit_transform(df['Shipping Mode'])\n",
        "\n",
        "# Category\n",
        "df['Category'] = le.fit_transform(df['Category'])"
      ],
      "metadata": {
        "id": "oYI-V_1-gn1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "9-GWa1ZugtDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objs as go\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "\n",
        "temp = df[\"Category\"].value_counts()\n",
        "df1 = pd.DataFrame({'Category': temp.index,'values': temp.values})\n",
        "\n",
        "# Define a list of colors for the bars\n",
        "colors = ['red', 'blue', 'green']\n",
        "\n",
        "traces = []\n",
        "for i, category in enumerate(df1['Category']):\n",
        "    if category == 1:\n",
        "        name = \"Regular\"\n",
        "    elif category == 0:\n",
        "        name = \"Suspected\"\n",
        "    else:\n",
        "        name = \"Fraud\"\n",
        "    trace = go.Bar(\n",
        "        x=[name], y=[df1.loc[i, 'values']],\n",
        "        name=name,\n",
        "        marker=dict(color=colors[i]),\n",
        "        text=[df1.loc[i, 'values']],\n",
        "        legendgroup=\"group\"\n",
        "    )\n",
        "    traces.append(trace)\n",
        "\n",
        "layout = dict(title='Credit Card Fraud Class - data unbalance',\n",
        "              xaxis=dict(title='Class', showticklabels=True), \n",
        "              yaxis=dict(title='Number of transactions'),\n",
        "              hovermode='closest', width=600,\n",
        "              showlegend=True\n",
        "             )\n",
        "fig = go.Figure(data=traces, layout=layout)\n",
        "iplot(fig, filename='class')"
      ],
      "metadata": {
        "id": "lc9I9cFcguQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Plot\n",
        "\n",
        "df.corr().style.background_gradient(cmap='coolwarm').set_properties(**{'max_width': '50px'})"
      ],
      "metadata": {
        "id": "eTME0u-ygwMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the Data"
      ],
      "metadata": {
        "id": "qxesqjQ3g1vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df.drop(['Category Name', 'Customer City',\n",
        "       'Customer Country', 'Customer Fname', 'Customer Id', 'Customer Lname',\n",
        "       'Customer State',\t'Customer Street', 'Department Name', \n",
        "       'Market', 'Order City', 'Order Country', 'Order Customer Id', 'Order Region',\t\n",
        "       'Order State', 'Product Image',\t'Product Name',\n",
        "       'shipping date (DateOrders)', 'order date (DateOrders)', 'Category Id', 'Customer Zipcode', \n",
        "       'Department Id', 'Latitude',\t'Longitude', 'Order Id',\t'Order Item Cardprod Id',\n",
        "       'Order Item Id', 'Product Card Id', 'Product Category Id'], axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "0ZeocGMtg6Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split  \n",
        "\n",
        "X = df.drop(['Category'], axis=1) #Not scaled\n",
        "y = df['Category']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True)"
      ],
      "metadata": {
        "id": "gWBPtWgmhDds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "s = StandardScaler()\n",
        "\n",
        "X_train = s.fit_transform(X_train)\n",
        "X_test = s.transform(X_test)"
      ],
      "metadata": {
        "id": "VmakGM-9hILk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA"
      ],
      "metadata": {
        "id": "x6VsbaZXhAFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize a PCA object\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA object to the data\n",
        "pca.fit(X_train)\n",
        "\n",
        "# Create a scree plot\n",
        "num_components = len(pca.explained_variance_ratio_)\n",
        "plt.plot(np.arange(1, num_components+1), pca.explained_variance_ratio_, 'ro-', linewidth=2)\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Proportion of Variance Explained')\n",
        "plt.show()\n",
        "\n",
        "# Determine the number of components to keep\n",
        "variance_threshold = 0.95\n",
        "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "num_components_to_keep = np.argmax(cumulative_variance_ratio >= variance_threshold) + 1\n",
        "\n",
        "print(f'\\n Number of components to keep: {num_components_to_keep}')\n",
        "\n",
        "# Transform the data using the chosen number of components\n",
        "pca = PCA(n_components=num_components_to_keep)\n",
        "X_train_p = pca.fit_transform(X_train)\n",
        "X_test_p = pca.transform(X_test)"
      ],
      "metadata": {
        "id": "U7OxQZNVhKy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the cumulative variance explained\n",
        "\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "cum = np.insert(cumulative_variance_ratio, 0, 0)\n",
        "ylab = np.insert(np.cumsum(pca.explained_variance_ratio_), 0, 0)\n",
        "\n",
        "plt.plot(cum, 'ro-', linewidth=2)\n",
        "plt.title('Cumulative Variance Explained')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Variance Explained')\n",
        "plt.yticks(ylab)\n",
        "plt.xticks(np.arange(0, 21))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QjzxQbnEhNpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Heatmap loadings\n",
        "fig = plt.figure(figsize=(20, 10))\n",
        "sns.heatmap(pca.components_, cmap='coolwarm', annot=True, cbar=False)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Principal Components')\n",
        "plt.title('PCA Loadings Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yq9z5t2ghPL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the principal components as vectors in the original feature space\n",
        "pc_vectors = pca.components_\n",
        "\n",
        "# Get the names of the original columns\n",
        "column_names = X.columns\n",
        "\n",
        "# Print the names of the columns chosen as principal components\n",
        "num_pcs = pc_vectors.shape[0]\n",
        "for i in range(num_pcs):\n",
        "    pc_name = f'PC{i+1}'\n",
        "    pc_loadings = pc_vectors[i]\n",
        "    relevant_columns = column_names[np.abs(pc_loadings) >= 0.40]\n",
        "    print(f'{pc_name}:\\n {relevant_columns.tolist()}, \\n {pc_loadings[np.abs(pc_loadings) >= 0.40]} \\n ')"
      ],
      "metadata": {
        "id": "vu9QVG5whQpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "XtVZwv9uhUIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X_train = pd.DataFrame(X_train)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "y_train = np.ravel(y_train)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(type(X), type(y))"
      ],
      "metadata": {
        "id": "36m8DYu5hVjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "fraud = StackingClassifier(estimators=[('lr', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=2000, class_weight='balanced')),\n",
        "                                      ('dt', DecisionTreeClassifier(max_depth=4))],\n",
        "                                              final_estimator=DecisionTreeClassifier(max_depth=5))\n",
        "\n",
        "fraud1 = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=2000, class_weight='balanced')\n",
        "\n",
        "regular = GradientBoostingClassifier(n_estimators=100, learning_rate=0.01, max_depth=4, subsample=0.8)\n",
        "\n",
        "regular2 = SGDClassifier(loss='log_loss', penalty='elasticnet', alpha=0.0001, l1_ratio=1, max_iter=1000)\n",
        "\n",
        "suspected = DecisionTreeClassifier(class_weight='balanced', max_depth= 4)\n",
        "\n",
        "reg_sus = XGBClassifier(objective='multi:softmax', num_class=3, max_depth=4, subsample=0.8,\n",
        "                    colsample_bytree=0.8, learning_rate=0.3)"
      ],
      "metadata": {
        "id": "UvwtbzdahXWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voting2class1"
      ],
      "metadata": {
        "id": "M7sZ-P6chbUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Initialize the voting classifier with the base models\n",
        "model = VotingClassifier(estimators=[('fraud', fraud1), ('reg_sus', reg_sus)], voting='soft')\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "with open('/content/drive/MyDrive/University/Deloitte/models/Voting2class1.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "dAOG-tN0hd5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voting2class"
      ],
      "metadata": {
        "id": "FXkAv1yhhgCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Initialize the voting classifier with the base models\n",
        "model = VotingClassifier(estimators=[('fraud', fraud), ('reg_sus', reg_sus)], voting='hard')\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "with open('/content/drive/MyDrive/University/Deloitte/models/Voting2class.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "6EzhA_Xnhhx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BaggingLogistic"
      ],
      "metadata": {
        "id": "iWGK1GcMhj0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "model = BaggingClassifier(estimator=LogisticRegression(max_iter=1000, multi_class='multinomial'),\n",
        "                  n_estimators=30)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "with open('/content/drive/MyDrive/University/Deloitte/models/BaggingLogistic.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "qFdUBOEQhjZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try the model over 100 split"
      ],
      "metadata": {
        "id": "Nnar40FVh9Nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix, recall_score\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# Standardize the data and split it into training and test sets\n",
        "s = StandardScaler()\n",
        "\n",
        "recall_scores = []\n",
        "precision_scores = []\n",
        "f1_scores = []\n",
        "accuracy_scores = []\n",
        "fraud_recall = []\n",
        "suspected_recall = []\n",
        "regular_recall = [] \n",
        "low = []\n",
        "avg_conf_matrix = np.zeros((3, 3))\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "#StackingXGB\n",
        "\n",
        "with open('/content/drive/MyDrive/University/Deloitte/bests/Voting2class.pkl', 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "for i in range(1, 101):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True)\n",
        "\n",
        "  X_train = s.fit_transform(X_train)\n",
        "  X_test = s.transform(X_test)\n",
        "\n",
        "    # PCA\n",
        "  pca = PCA()\n",
        "  pca.fit(X_train)\n",
        "\n",
        "  cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "  num_components_to_keep = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
        "\n",
        "  pca = PCA(n_components=num_components_to_keep)\n",
        "  X_train = pca.fit_transform(X_train)\n",
        "  X_test = pca.transform(X_test)\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  recalls = []\n",
        "  for j in range(conf_matrix.shape[0]):\n",
        "      tp = conf_matrix[j,j]\n",
        "      fn = np.sum(conf_matrix[j,:]) - tp\n",
        "      recall = tp / (tp + fn)\n",
        "      recalls.append(recall)\n",
        "\n",
        "  recall_scores.append(recall_score(y_test, y_pred, average=\"macro\"))\n",
        "  fraud_recall.append(recalls[0])\n",
        "  regular_recall.append(recalls[1])\n",
        "  suspected_recall.append(recalls[2])\n",
        "\n",
        "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "  avg_conf_matrix += conf_matrix\n",
        "\n",
        "  if i % 10 == 0:\n",
        "      print(f'Iteration: {i}')\n",
        "      print(f'Fraud Recall {round(np.average(fraud_recall), 4)}, {round(np.std(fraud_recall), 4)}')\n",
        "      print(f'Suspected Recall {round(np.average(suspected_recall), 4)}, {round(np.std(suspected_recall), 4)}')\n",
        "      print(f'Regular Recall {round(np.average(regular_recall), 4)}, {round(np.std(regular_recall), 4)}')\n",
        "      print(f'Total Recall {round(np.average(recall_scores), 4)}, {round(np.std(recall_scores), 4)} \\n')\n",
        "\n",
        "  if recalls[0] < 0.7:\n",
        "    low.append(round(recalls[0], 4))\n",
        "  \n",
        "print(f'\\n Fraud Recall: {round(np.average(fraud_recall), 4)}, std: {round(np.std(fraud_recall), 4)}, Under 0.7: {len(low)}, {low}\\n Suspected Recall: {round(np.average(suspected_recall), 4)}, std: {round(np.std(suspected_recall), 4)}\\n Regular Recall: {round(np.average(regular_recall), 4)}, std: {round(np.std(regular_recall), 4)}\\n Total: {round(np.average(recall_scores), 4)}, std: {round(np.std(recall_scores), 4)}')\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "avg_conf_matrix /= 100\n",
        "print(\"\\n Average Confusion Matrix:\")\n",
        "print(avg_conf_matrix)\n",
        "\n",
        "print('\\n', model)\n"
      ],
      "metadata": {
        "id": "Ji-IRFm0iBed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "o5OPPT_Ohvnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Baseline Model**\n",
        "```python\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=2000, class_weight='balanced')\n",
        "```\n",
        "```python\n",
        "Fraud Recall: 0.9228, std: 0.1951\n",
        "Average Confusion Matrix: [749 42 2]\n",
        "```\n",
        "```python\n",
        "Fraud Recall: 0.9228, std: 0.1951\n",
        "Average Confusion Matrix: [749 42 2]\n",
        "```\n",
        "```python\n",
        "Fraud Recall: 0.9228, std: 0.1951\n",
        "Average Confusion Matrix: [749 42 2]\n",
        "```\n",
        "\n",
        "\n",
        "**Best Fraud**\n",
        "\n",
        "*Voting2class*\n",
        "```python\n",
        "VotingClassifier(estimators=[('fraud',\n",
        "                              StackingClassifier(estimators=[('lr', LogisticRegression(class_weight='balanced', max_iter=2000, multi_class='multinomial')),\n",
        "                                                             ('dt', DecisionTreeClassifier(max_depth=4))],\n",
        "                                                              final_estimator=DecisionTreeClassifier(max_depth=5))),\n",
        "                             ('reg_sus',\n",
        "                              XGBClassifier(learning_rate=0.3, max_depth=4, n_estimators=100, num_class=3, objective='multi:softmax'))])\n",
        "```\n",
        "```python\n",
        "Fraud Recall: 0.9228, std: 0.1951\n",
        "Average Confusion Matrix: [749 42 2]\n",
        "```\n",
        "\n",
        "**Best Suspected**\n",
        "\n",
        "*Voting2class1*\n",
        "```python\n",
        "VotingClassifier(estimators=[('fraud',\n",
        "                              LogisticRegression(class_weight='balanced', max_iter=2000, multi_class='multinomial')),\n",
        "                             ('reg_sus',\n",
        "                              XGBClassifier(colsample_bytree=0.8,\n",
        "                                            learning_rate=0.3, max_depth=4, n_estimators=100, num_class=3, objective='multi:softmax'))], \n",
        "                  voting='soft')\n",
        "```\n",
        "```python\n",
        "Suspected Recall: 0.8843, std: 0.1285\n",
        "Average Confusion Matrix: [24 787 6190]\n",
        "```\n",
        "\n",
        "**Best Regular**\n",
        "\n",
        "*Bagging Logistic Model:*\n",
        "```python\n",
        "BaggingClassifier(estimator=LogisticRegression(max_iter=1000, multi_class='multinomial'),\n",
        "                  n_estimators=30)\n",
        "```\n",
        "```python\n",
        "Regular Recall: 0.9585, std: 0.0422\n",
        "Average Confusion Matrix: [466  27115  708]\n",
        "```\n",
        "\n",
        "**Best Overall**\n",
        "1. *Voting2class1* \n",
        "\n",
        "more balanced between the 3 recalls\n",
        "```python\n",
        "VotingClassifier(estimators=[('fraud',\n",
        "                              LogisticRegression(class_weight='balanced', max_iter=2000, multi_class='multinomial')),\n",
        "                             ('reg_sus',\n",
        "                              XGBClassifier(colsample_bytree=0.8,\n",
        "                                            learning_rate=0.3, max_depth=4, n_estimators=100, num_class=3, objective='multi:softmax'))], \n",
        "                  voting='soft')\n",
        "```\n",
        "```python\n",
        "Fraud Recall: 0.8637, std: 0.2231\n",
        "```\n",
        "```python\n",
        "Suspected Recall: 0.8913, std: 0.1258\n",
        "```\n",
        "```python\n",
        "Regular Recall: 0.9366, std: 0.0675\n",
        "```\n",
        "```python\n",
        "Total Recall: 0.8972, std: 0.1322\n",
        "```\n",
        "```python\n",
        " Average Confusion Matrix:\n",
        "[[701  87  24]\n",
        " [235  26500  1560]\n",
        " [25  736  6240]]\n",
        "```\n",
        "\n",
        "2. *Voting2class* \n",
        "\n",
        "Better on Regular and Fraud, less in Suspected\n",
        "```python\n",
        "VotingClassifier(estimators=[('fraud', StackingClassifier(estimators=[('lr', LogisticRegression(class_weight='balanced', max_iter=2000, multi_class='multinomial')), \n",
        "                                                                      ('dt', DecisionTreeClassifier(max_depth=4))], \n",
        "                                                          final_estimator=DecisionTreeClassifier(max_depth=5))),\n",
        "                             ('reg_sus',\n",
        "                              XGBClassifier(learning_rate=0.3, max_depth=4 n_estimators=100, num_class=3, objective='multi:softmax'))])\n",
        "```\n",
        "\n",
        "```python\n",
        "Fraud Recall: 0.9387, std: 0.1318\t\n",
        "```\n",
        "```python\n",
        "Suspected Recall: 0.7848, std: 0.0763 \n",
        "```\n",
        "```python\n",
        "Regular Recall: 0.9541, std: 0.0395\n",
        "```\n",
        "```python\n",
        "Total Recall: 0.8925, std: 0.0708\n",
        "```\n",
        "```python\n",
        " Average Confusion Matrix:\n",
        "[[762  5  0]\n",
        " [625  27000  674]\n",
        " [5  1500  5490]]\n",
        " ```"
      ],
      "metadata": {
        "id": "rqSSlsMCh1G-"
      }
    }
  ]
}