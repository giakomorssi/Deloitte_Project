{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giakomorssi/Deloitte_Project/blob/main/03_FraudDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0315A0JjvY-"
      },
      "source": [
        "# Import the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffbsL3U8jkV5",
        "outputId": "b76ee6e6-343d-447a-bfc6-c5aa67afa7a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Runtime switched to GPU\n",
            "GPU device found: /device:GPU:0\n",
            "Default GPU Device: /device:GPU:0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Change Colab runtime to GPU\n",
        "import os\n",
        "os.environ['COLAB_TPU_ADDR'] = ''\n",
        "os.environ['COLAB_GPU_ALLOC'] = '1'\n",
        "os.environ['COLAB_GPU'] = '1'\n",
        "print(\"Runtime switched to GPU\")\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "if not tf.test.gpu_device_name():\n",
        "    print('GPU device not found')\n",
        "else:\n",
        "    print('GPU device found:', tf.test.gpu_device_name())\n",
        "\n",
        "# This code sets the runtime to use the GPU if available\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"Please install GPU version of TF\")\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "!pip install category_encoders -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aboYVnjYjxh5"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('SupplyChainDataset_eda.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1wlvoXlj8Mu"
      },
      "source": [
        "# Category Column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P3--qOaj_Nk"
      },
      "source": [
        "\n",
        "1. **COMPLETE:** The order or transaction has been successfully fulfilled and completed.\n",
        "2. **PENDING**: The order or transaction is still in progress and has not yet been completed.\n",
        "3. **CLOSED**: The order or transaction has been closed or terminated for some reason, such as a return or cancellation.\n",
        "4. **PENDING_PAYMENT**: The order or transaction is awaiting payment before it can be processed.\n",
        "5. **CANCELED**: The order or transaction has been canceled by the customer or the seller for some reason.\n",
        "6. **PROCESSING**: The order or transaction is being processed by the seller or merchant.\n",
        "7. **SUSPECTED_FRAUD**: The order or transaction is under review due to suspected fraudulent activity.\n",
        "8. **ON_HOLD**: The order or transaction has been placed on hold for some reason, such as a delay in shipping or a credit hold.\n",
        "9. **PAYMENT_REVIEW**: The payment for the order or transaction is under review by the payment processor or financial institution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PErUmIXSj-dg"
      },
      "outputs": [],
      "source": [
        "# Regular -> Complete, Pending, Pending_Payment, Processing\n",
        "# Suspected -> Closed, Canceled, On_Hold, Payment_Review\n",
        "# Fraud -> Suspected_Fraud\n",
        "\n",
        "# define dictionaries to map status values to categories\n",
        "regular_dict = {'COMPLETE': 'Regular', 'PENDING': 'Regular', 'PENDING_PAYMENT': 'Regular', 'PROCESSING': 'Regular'}\n",
        "suspected_dict = {'CLOSED': 'Suspected', 'CANCELED': 'Suspected', 'ON_HOLD': 'Suspected', 'PAYMENT_REVIEW': 'Suspected'}\n",
        "fraud_dict = {'SUSPECTED_FRAUD': 'Fraud'}\n",
        "\n",
        "# create a function to map status values to categories\n",
        "def map_category(status):\n",
        "    if status in regular_dict:\n",
        "        return regular_dict[status]\n",
        "    elif status in suspected_dict:\n",
        "        return suspected_dict[status]\n",
        "    elif status in fraud_dict:\n",
        "        return fraud_dict[status]\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "# apply the function to the 'status' column to create a new 'category' column\n",
        "df['Category'] = df['Order Status'].apply(map_category)\n",
        "\n",
        "print('Regular: ', len([x for x in df['Category'] if x == 'Regular']), '\\n')\n",
        "print('Suspected: ', len([x for x in df['Category'] if x == 'Suspected']), '\\n')\n",
        "print('Fraud: ', len([x for x in df['Category'] if x == 'Fraud']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jPOt1hs2CKN"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "\n",
        "temp = df[\"Category\"].value_counts()\n",
        "df1 = pd.DataFrame({'Category': temp.index,'values': temp.values})\n",
        "\n",
        "# define the colors for each category\n",
        "colors = ['red', 'blue', 'green']\n",
        "\n",
        "# create the bar plot with colors and legend\n",
        "sns.set_style('whitegrid')\n",
        "ax = sns.barplot(x='Category', y='values', data=df1, palette=colors)\n",
        "\n",
        "# set the axis labels and title\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Category Counts')\n",
        "\n",
        "# add the number on top of each bar\n",
        "for i, v in enumerate(df1['values']):\n",
        "    ax.text(i, v, str(v), ha='center', va='bottom')\n",
        "\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn7gWskckHxx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df.drop(['Customer City', 'Order Status'], axis = 1, inplace = True)\n",
        "\n",
        "df['shipping date (DateOrders)'] = pd.to_datetime(df['shipping date (DateOrders)'])\n",
        "df['order date (DateOrders)'] = pd.to_datetime(df['order date (DateOrders)'])\n",
        "\n",
        "df['shipping date (DateOrders)'] = df['shipping date (DateOrders)'].apply(lambda x: x.timestamp())\n",
        "df['order date (DateOrders)'] = df['order date (DateOrders)'].apply(lambda x: x.timestamp())\n",
        "\n",
        "df.to_csv('df_fraud.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_JKQJl9kC5z"
      },
      "source": [
        "# Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eezGwCHSkNB9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split  \n",
        "\n",
        "X = df.drop(['Category'], axis=1)\n",
        "y = df['Category']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaz4tUWZkFKd"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Shipping Mode\n",
        "custom_order = ['Same Day', 'First Class', 'Second Class', 'Standard Class']\n",
        "le.fit(custom_order)\n",
        "X_train['Shipping Mode'] = le.fit_transform(X_train['Shipping Mode'])\n",
        "X_test['Shipping Mode'] = le.transform(X_test['Shipping Mode'])\n",
        "\n",
        "# Delivery Status\n",
        "# Define the custom order\n",
        "custom_order = ['Shipping on time', 'Advance shipping', 'Late delivery', 'Shipping canceled']\n",
        "le.fit(custom_order)\n",
        "X_train['Delivery Status'] = le.fit_transform(X_train['Delivery Status'])\n",
        "X_test['Delivery Status'] = le.transform(X_test['Delivery Status'])\n",
        "\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6My5oGc8x9Dr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from category_encoders import LeaveOneOutEncoder\n",
        "\n",
        "# initialize the encoder\n",
        "enc = LeaveOneOutEncoder(cols=['Order City'])\n",
        "\n",
        "# fit and transform the entire dataset\n",
        "X_train = enc.fit_transform(X_train, y_train)\n",
        "X_test = enc.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wjnsd189n8vM"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Select columns for one-hot encoding\n",
        "one_hot_cols = [0, 7, 8, 11]\n",
        "\n",
        "# Fit one-hot encoder to training data\n",
        "one_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "\n",
        "# Apply one-hot encoder to training and test data\n",
        "X_train_one_hot = one_hot_encoder.fit_transform(X_train.iloc[:, one_hot_cols])\n",
        "X_test_one_hot = one_hot_encoder.transform(X_test.iloc[:, one_hot_cols])\n",
        "\n",
        "# Remove original columns from training and test data\n",
        "X_train = X_train.drop(X_train.columns[one_hot_cols], axis=1)\n",
        "X_test = X_test.drop(X_test.columns[one_hot_cols], axis=1)\n",
        "\n",
        "# Concatenate one-hot encoded columns with remaining data\n",
        "X_train = pd.concat([pd.DataFrame(X_train_one_hot.toarray()), X_train.reset_index(drop=True)], axis=1)\n",
        "X_test = pd.concat([pd.DataFrame(X_test_one_hot.toarray()), X_test.reset_index(drop=True)], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oohbPMutkzUw"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)\n",
        "\n",
        "s = StandardScaler()\n",
        "\n",
        "X_train.iloc[:, 23:] = s.fit_transform(X_train.iloc[:, 23:])\n",
        "X_test.iloc[:, 23:] = s.transform(X_test.iloc[:, 23:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZKOgCbeELOn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = pd.DataFrame(X_test)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "y_train = np.ravel(y_train)\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB47s_04FOjN"
      },
      "source": [
        "# PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au3huJShFQyl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca_train = X_train.iloc[:, 23:]\n",
        "\n",
        "# Initialize a PCA object\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA object to the data\n",
        "pca.fit(X_train.iloc[:, 23:])\n",
        "\n",
        "# Determine the number of components to keep\n",
        "variance_threshold = 0.95\n",
        "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "num_components_to_keep = np.argmax(cumulative_variance_ratio >= variance_threshold) + 1\n",
        "\n",
        "# Elbow plot\n",
        "\n",
        "plt.plot(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_, 'bo-', linewidth=2)\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Scree Plot')\n",
        "plt.show()\n",
        "\n",
        "print(f'\\n Number of components to keep: {num_components_to_keep}')\n",
        "\n",
        "# Transform the data using the chosen number of components\n",
        "pca = PCA(n_components=num_components_to_keep)\n",
        "pca_train = pca.fit_transform(X_train.iloc[:, 23:])\n",
        "pca_test = pca.transform(X_test.iloc[:, 23:])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Plotting the cumulative variance explained by principal components\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(cumulative_variance_ratio)\n",
        "ax.set_xlabel('Number of Principal Components')\n",
        "ax.set_ylabel('Cumulative Variance Ratio')\n",
        "ax.set_title('Cumulative Variance Explained by Principal Components')\n",
        "\n",
        "# Indicating the cumulative variance up to the chosen number of components\n",
        "num_components = num_components_to_keep\n",
        "ax.axhline(y=cumulative_variance_ratio[num_components], color='r', linestyle='--')\n",
        "ax.text(0.5, cumulative_variance_ratio[num_components]-0.05, \n",
        "        f'{cumulative_variance_ratio[num_components]:.6f}', color='r')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AECMx_RuysLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "cols = X_train.iloc[:, 23:].columns\n",
        "# Heatmap loadings\n",
        "fig = plt.figure(figsize=(18, 10))\n",
        "sns.heatmap(np.round(pca.components_, 2), cmap='coolwarm', annot=True, cbar=False, xticklabels=cols)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Principal Components')\n",
        "plt.title('PCA Loadings Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E_WWuSd-Usrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the principal components as vectors in the original feature space\n",
        "pc_vectors = pca.components_\n",
        "\n",
        "# Get the names of the original columns\n",
        "column_names = cols\n",
        "\n",
        "# Print the names of the columns chosen as principal components\n",
        "num_pcs = pc_vectors.shape[0]\n",
        "for i in range(num_pcs):\n",
        "    pc_name = f'PC{i+1}'\n",
        "    pc_loadings = pc_vectors[i]\n",
        "    relevant_col_indices = np.where(np.abs(pc_loadings) >= 0.60)[0]\n",
        "    relevant_columns = column_names[relevant_col_indices]\n",
        "    relevant_loadings = pc_loadings[relevant_col_indices]\n",
        "    print(f'{pc_name}:')\n",
        "    for j in range(len(relevant_columns)):\n",
        "        print(f'{relevant_columns[j]}: {relevant_loadings[j]}')\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "G8CG596VUwFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iSefjj0ZhDo"
      },
      "outputs": [],
      "source": [
        "onehot_data_train = X_train.iloc[:, :23]\n",
        "onehot_data_test = X_test.iloc[:, :23]\n",
        "\n",
        "X_train = pd.concat([pd.DataFrame(pca_train), pd.DataFrame(onehot_data_train)], axis=1)\n",
        "X_test = pd.concat([pd.DataFrame(pca_test), pd.DataFrame(onehot_data_test)], axis=1)\n",
        "\n",
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = pd.DataFrame(X_test)\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.columns = range(len(X_train.columns))\n",
        "X_test.columns = range(len(X_test.columns))"
      ],
      "metadata": {
        "id": "T7STa0ElufmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-NJnauSkf84"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score\n",
        "\n",
        "lr = LogisticRegression(class_weight='balanced')\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(recall_score(y_test, y_pred, average=None))"
      ],
      "metadata": {
        "id": "cLB5rJWerYFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(class_weight='balanced', max_depth=4)\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(recall_score(y_test, y_pred, average=None))"
      ],
      "metadata": {
        "id": "E9Kn0hhCrp0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb = XGBClassifier()\n",
        "\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = xgb.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(recall_score(y_test, y_pred, average=None))"
      ],
      "metadata": {
        "id": "LQ1PrhMSrtND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "lgbm = LGBMClassifier(max_depth=4, n_estimators=1000)\n",
        "\n",
        "lgbm.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lgbm.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(recall_score(y_test, y_pred, average=None))"
      ],
      "metadata": {
        "id": "KvSH9qG4ulKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier(class_weight='balanced')\n",
        "\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "y_pred = dt.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(recall_score(y_test, y_pred, average=None))"
      ],
      "metadata": {
        "id": "9IHBEX5Zv2k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "import pickle\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score, confusion_matrix\n",
        "\n",
        "vc = VotingClassifier(estimators=[('xgb', XGBClassifier()),\n",
        "                                     ('lr', LogisticRegression(class_weight='balanced', max_iter=1000))],\n",
        "                         voting='soft')\n",
        "\n",
        "vc.fit(X_train, y_train)\n",
        "\n",
        "with open('vc.pkl', 'wb') as file:\n",
        "    pickle.dump(vc, file)\n",
        "\n",
        "y_pred = vc.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(recall_score(y_test, y_pred, average=None))"
      ],
      "metadata": {
        "id": "2s5BodFHyVQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VEPI75g2_Ly"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score\n",
        "import pickle\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)\n",
        "\n",
        "print(vc)\n",
        "\n",
        "y_pred = vc.predict(X_test)\n",
        "\n",
        "print('Confusion Matrix\\n', confusion_matrix(y_test, vc.predict(X_test)), '\\n')\n",
        "print('Recalls', recall_score(y_test, y_pred, average=None))\n",
        "print('Precisions', precision_score(y_test, y_pred, average=None))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "t0315A0JjvY-",
        "lYRBHmf-j3_o",
        "tB47s_04FOjN",
        "plqxS6GvUrRO"
      ],
      "provenance": [],
      "mount_file_id": "19Tbiy_yJN-lySn3fdPoeAjtL9g-yHM_7",
      "authorship_tag": "ABX9TyPXNTnqT3y6TgxiFqxuLakM",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}